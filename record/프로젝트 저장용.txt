1차 학습
입력 데이터 형식:
{
  "personality": "존칭형",       // "평범형", "존칭형", "하대형", "단답형", "너스레형", "야성형"
  "adventurerType": "전사",     // "전사", "궁수", "마법사", "도적"
  "affectionLevel": 2,          // 0=매우 낮음, 1=낮음, 2=보통, 3=높음, 4=매우 높음
  "visitCount": 2,              // 0=첫방문, 1=초면, 2=아는사이, 3=친구, 4=단골
  "context": {
    "lastResult": 2,            // 0=null(첫방문), 1=실패, 2=성공, 3=대성공
    "lastTypeMatch": 1          // 0=나쁨, 1=보통, 2=좋음 (이전 대여 무기와의 상성)
  }
}

모델: EXAONE-3.5-2.4B 학습률:2e-4 epoch:3 dropout:0.05
{'loss': 4.6088, 'grad_norm': 22.80320930480957, 'learning_rate': 0.0001872340425531915, 'epoch': 0.21}
{'loss': 2.2326, 'grad_norm': 23.368995666503906, 'learning_rate': 0.00017304964539007094, 'epoch': 0.42}                                                                                                                  
{'loss': 1.1016, 'grad_norm': 12.463349342346191, 'learning_rate': 0.00015886524822695037, 'epoch': 0.63}                                                                                                                  
{'loss': 0.7764, 'grad_norm': 13.498905181884766, 'learning_rate': 0.0001446808510638298, 'epoch': 0.84}                                                                                                                   
{'loss': 0.6398, 'grad_norm': 11.978620529174805, 'learning_rate': 0.00013191489361702127, 'epoch': 1.05}                                                                                                                  
{'loss': 0.5849, 'grad_norm': 10.862421989440918, 'learning_rate': 0.00011773049645390071, 'epoch': 1.26}                                                                                                                  
{'loss': 0.519, 'grad_norm': 9.910893440246582, 'learning_rate': 0.00010354609929078013, 'epoch': 1.47}                                                                                                                    
{'loss': 0.5099, 'grad_norm': 12.074780464172363, 'learning_rate': 8.936170212765958e-05, 'epoch': 1.68}                                                                                                                   
{'loss': 0.4811, 'grad_norm': 388.5758972167969, 'learning_rate': 8.085106382978723e-05, 'epoch': 1.88}                                                                                                                    
{'loss': 0.4379, 'grad_norm': 11.574589729309082, 'learning_rate': 6.666666666666667e-05, 'epoch': 2.09}                                                                                                                   
{'loss': 0.3821, 'grad_norm': 11.433417320251465, 'learning_rate': 5.24822695035461e-05, 'epoch': 2.3}                                                                                                                     
{'loss': 0.3925, 'grad_norm': 9.021378517150879, 'learning_rate': 3.829787234042553e-05, 'epoch': 2.51}                                                                                                                    
{'loss': 0.4029, 'grad_norm': 12.193809509277344, 'learning_rate': 2.4113475177304965e-05, 'epoch': 2.72}                                                                                                                  
{'loss': 0.4031, 'grad_norm': 10.77366828918457, 'learning_rate': 9.929078014184398e-06, 'epoch': 2.93}  


학습결과로 추론해본 결과
1차 추론
max_new_tokens=50(답변 길이를 여유있게) temperature=0.7(다양성) repetition_penalty=1.2(같은 단어 반복 방지 강화) top_p=0.9(단어 누적 확률 임계값) do_sample=True(랜덤성)
입력 조건: 타입: 전사, 성격: 존칭형, 호감도: 매우높음, 방문: 단골, 이전결과: 대성공, 상성: 좋음
모험가 답변: 안녕하세요! 저번 검 덕분에 크게 승리했습니다. 감사합니다.

입력 조건: 타입: 도적, 성격: 하대형, 호감도: 매우낮음, 방문: 초면, 이전결과: 실패, 상성: 나쁨
모험가 답변: 크흠. 저번 단검은 좀 부실했어. 튼튼하고 날이 날카로운 걸로 줘 봐.

입력 조건: 타입: 마법사, 성격: 단답형, 호감도: 낮음, 방문: 아는사이, 이전결과: 성공, 상성: 보통
모험가 답변: 단검. 빌려줘.

2차 추론
max_new_tokens=100 temperature=0.85 repetition_penalty=1.5 top_p=0.9 do_sample=True
입력 조건: 타입: 전사, 성격: 존칭형, 호감도: 매우높음, 방문: 단골, 이전결과: 대성공, 상성: 좋음
모험가 답변: 안녕합니다! 저번 검 덕분에 승리했습니다만용 감사드립니다. 오늘도 부탁드리겠습니까?

입력 조건: 타입: 도적, 성격: 하대형, 호감도: 매우낮음, 방문: 초면, 이전결과: 실패, 상성: 나쁨
모험가 답변: 크흠! 저번엔 운이 없어서 failure였지? 이번에는 좀 더 좋다야!!

입력 조건: 타입: 마법사, 성격: 단답형, 호감도: 낮음, 방문: 아는사이, 이전결과: 성공, 상성: 보통
모험가 답변: 지팡이 줘. 저번 것보다 좋다.

3차 추론
max_new_tokens=60 temperature=0.5 repetition_penalty=1.2 top_p=0.9 do_sample=False
입력 조건: 타입: 전사, 성격: 존칭형, 호감도: 매우높음, 방문: 단골, 이전결과: 대성공, 상성: 좋음
모험가 답변: 안녕하십니까! 저번 검 덕분에 성공했습니다. 오늘도 부탁드립니다!

입력 조건: 타입: 도적, 성격: 하대형, 호감도: 매우낮음, 방문: 초면, 이전결과: 실패, 상성: 나쁨
모험가 답변: 야! 저번 단검은 뭐였지? 이번엔 더 좋은 거 줘봐.

입력 조건: 타입: 마법사, 성격: 단답형, 호감도: 낮음, 방문: 아는사이, 이전결과: 성공, 상성: 보통
모험가 답변: 지팡이. 저번 거 좋았어. 오늘도 같아.

4차 추론
max_new_tokens=60 temperature=0.5 repetition_penalty=1.2 top_p=0.9 do_sample=True
입력 조건: 타입: 전사, 성격: 존칭형, 호감도: 매우높음, 방문: 단골, 이전결과: 대성공, 상성: 좋음
모험가 답변: 안녕하세요! 저번 단검 덕분에 큰 성공했습니다! 오늘도 부탁드립니다.

입력 조건: 타입: 도적, 성격: 하대형, 호감도: 매우낮음, 방문: 초면, 이전결과: 실패, 상성: 나쁨
모험가 답변: 야! 저번 단검은 뭐였지? 이번엔 좀 더 튼튼한 거 줘.

입력 조건: 타입: 마법사, 성격: 단답형, 호감도: 낮음, 방문: 아는사이, 이전결과: 성공, 상성: 보통
모험가 답변: 지팡이. 저번 것처럼 튼튼하고.

5차 추론
max_new_tokens=60 temperature=0.3 repetition_penalty=1.1 top_p=0.7 do_sample=True
입력 조건: 타입: 전사, 성격: 존칭형, 호감도: 매우높음, 방문: 단골, 이전결과: 대성공, 상성: 좋음
모험가 답변: 안녕하십니까! 저번 검 덕분에 성공했습니다. 오늘도 부탁드립니다!

입력 조건: 타입: 도적, 성격: 하대형, 호감도: 매우낮음, 방문: 초면, 이전결과: 실패, 상성: 나쁨
모험가 답변: 야! 저번 단검은 너무 무거웠어. 좀 가벼운 거 없어?

입력 조건: 타입: 마법사, 성격: 단답형, 호감도: 낮음, 방문: 아는사이, 이전결과: 성공, 상성: 보통
모험가 답변: 지팡이. 저번 거 좋았어.


[품질 개선 계획]
현상: 입력 변수(lastTypeMatch)의 모호성으로 인한 모델의 판단 저하 및 근거 없는 정보 생성(Hallucination) 확인.
원인: 훈련 데이터 내 특정 단어의 빈도 불균형 및 짧은 학습 단계로 인한 문맥 파악 미흡.
대책: 데이터 증강(Data Augmentation)을 통해 직업별 고유 키워드 매칭을 강화하고, LoRA Rank를 상향하여 모델의 표현력을 개선할 예정.
또한 데이터 구조를 변경해 한국어 학습에 더 특화되도록 변경.



데이터 형식 변경 후 2차 학습

입력 데이터 형식:
{
  "성격": "존칭형",           // 평범형, 존칭형, 하대형, 단답형, 너스레형, 야성형
  "모험가타입": "전사",       // 전사, 궁수, 마법사, 도적
  "나이": "20대",             // 10대 ~ 50대 이상
  "성별": "여성",             // 남성, 여성
  "모험가등급": "A급",        // S, A, B, C, D급 (태도와 위엄 결정)
  "방문횟수": "보통",         // 첫방문, 적음, 보통, 많음, 단골
  "이전_아이템": "검",        // 없음, 검, 도끼, 창, 단검, 활, 석궁, 망치, 몽둥이, 지팡이, 마법서
  "재방문간격": "최근",       // 없음, 최근, 보통, 오래됨 (대화의 시간적 맥락)
  "최근_의뢰": "대성공",      // 첫방문, 실패, 성공, 대성공 (현재 기분 상태)
  "인사말": "안녕하세요! 지난번 검 덕분에 의뢰가 잘 풀렸어요. 감사합니다!"
}

모델: EXAONE-3.5-2.4B 학습률:2e-4 epoch:3 dropout:0.05
{'loss': 4.1202, 'grad_norm': 22.170122146606445, 'learning_rate': 0.00018823529411764707, 'epoch': 0.19}
{'loss': 1.9919, 'grad_norm': 20.704383850097656, 'learning_rate': 0.00017516339869281047, 'epoch': 0.39}                                                                                                 
{'loss': 0.9416, 'grad_norm': 11.861666679382324, 'learning_rate': 0.00016209150326797388, 'epoch': 0.58}                                                                                                 
{'loss': 0.6605, 'grad_norm': 12.393548965454102, 'learning_rate': 0.00014901960784313728, 'epoch': 0.78}                                                                                                 
{'loss': 0.5489, 'grad_norm': 9.180813789367676, 'learning_rate': 0.00013594771241830065, 'epoch': 0.97}                                                                                                  
{'loss': 0.4992, 'grad_norm': 9.418176651000977, 'learning_rate': 0.00012287581699346406, 'epoch': 1.17}                                                                                                  
{'loss': 0.4635, 'grad_norm': 9.433621406555176, 'learning_rate': 0.00010980392156862746, 'epoch': 1.36}                                                                                                  
{'loss': 0.4629, 'grad_norm': 7.562619209289551, 'learning_rate': 9.673202614379085e-05, 'epoch': 1.56}                                                                                                   
{'loss': 0.4249, 'grad_norm': 8.612433433532715, 'learning_rate': 8.366013071895425e-05, 'epoch': 1.75}                                                                                                   
{'loss': 0.4027, 'grad_norm': 8.088541030883789, 'learning_rate': 7.058823529411765e-05, 'epoch': 1.95}                                                                                                   
{'loss': 0.3681, 'grad_norm': 7.620570182800293, 'learning_rate': 5.7516339869281044e-05, 'epoch': 2.14}                                                                                                  
{'loss': 0.3809, 'grad_norm': 8.115813255310059, 'learning_rate': 4.4444444444444447e-05, 'epoch': 2.34}                                                                                                  
{'loss': 0.3534, 'grad_norm': 8.793417930603027, 'learning_rate': 3.137254901960784e-05, 'epoch': 2.53}                                                                                                   
{'loss': 0.3681, 'grad_norm': 7.7639851570129395, 'learning_rate': 1.8300653594771242e-05, 'epoch': 2.73}                                                                                                 
{'loss': 0.3853, 'grad_norm': 7.802074909210205, 'learning_rate': 5.228758169934641e-06, 'epoch': 2.92}                                                                                                   
{'train_runtime': 195.5511, 'train_samples_per_second': 6.305, 'train_steps_per_second': 0.782, 'train_loss': 0.8152894662096609, 'epoch': 2.98}   


1차 추론
max_new_tokens=100 temperature=0.7 top_p=0.9 do_sample=True

[테스트 1] 첫 방문 전사
생성된 인사말: 안녕하세요. 첫 방문이라서 어떤 무기가 좋을지 잘 모르겠습니다.

[테스트 2] 단골 궁수 (활로 대성공)
생성된 인사말: 아~ 사장님, 저번 활 덕분에 client님이 주문한 무기를 정확히 맞추니까요? 이번에도 잘 좀 해주세요!

[테스트 3] 야성형 전사 (도끼로 실패)
생성된 인사말: 크하하! 저번 도끼 날이 너무 무뎌서 잘 안 들어갔어!

2차 추론
max_new_tokens=50 do_sample=False
[테스트 1] 첫 방문 전사
생성된 인사말: 안녕하세요. 처음 방문인데, 초보자용 검이 있을까요?

[테스트 2] 단골 궁수 (활로 대성공)
생성된 인사말: 사장님~ 저번 활 덕분에 사냥감 다 잡았어요! 정말 최고예요!

[테스트 3] 야성형 전사 (도끼로 실패)
생성된 인사말: 크흠! 저번 도끼는 날이 너무 무디더군. 제대로 된 걸로 줘!


3차 학습
LORA config의 r(rank)를 16->32(모델이 직업과 아이템, 말투 사이의 복잡한 관계를 더 세밀하게 기억할 수 있는 공간을 늘림), 
lola_alpha를 32->64(우리가 학습시킨 NPC지식이 모델 전체 답변에 미치는 목소리 크기를 키움)로 변경
{'loss': 3.6535, 'grad_norm': 24.14164161682129, 'learning_rate': 0.00018823529411764707, 'epoch': 0.19}
{'loss': 1.2943, 'grad_norm': 19.52777099609375, 'learning_rate': 0.00017516339869281047, 'epoch': 0.39}                                                                                                  
{'loss': 0.674, 'grad_norm': 9.19630241394043, 'learning_rate': 0.00016209150326797388, 'epoch': 0.58}                                                                                                    
{'loss': 0.544, 'grad_norm': 12.03078842163086, 'learning_rate': 0.00014901960784313728, 'epoch': 0.78}                                                                                                   
{'loss': 0.4897, 'grad_norm': 8.881753921508789, 'learning_rate': 0.00013594771241830065, 'epoch': 0.97}                                                                                                  
{'loss': 0.4472, 'grad_norm': 9.4404935836792, 'learning_rate': 0.00012679738562091503, 'epoch': 1.17}                                                                                                    
{'loss': 0.403, 'grad_norm': 6.9110260009765625, 'learning_rate': 0.00011372549019607843, 'epoch': 1.36}                                                                                                  
{'loss': 0.4049, 'grad_norm': 7.518772125244141, 'learning_rate': 0.00010065359477124183, 'epoch': 1.56}                                                                                                  
{'loss': 0.3865, 'grad_norm': 7.076858043670654, 'learning_rate': 8.758169934640524e-05, 'epoch': 1.75}                                                                                                   
{'loss': 0.3654, 'grad_norm': 6.679224967956543, 'learning_rate': 7.450980392156864e-05, 'epoch': 1.95}                                                                                                   
{'loss': 0.3284, 'grad_norm': 6.597250938415527, 'learning_rate': 6.143790849673203e-05, 'epoch': 2.14}                                                                                                   
{'loss': 0.3382, 'grad_norm': 7.080878257751465, 'learning_rate': 4.8366013071895424e-05, 'epoch': 2.34}                                                                                                  
{'loss': 0.3144, 'grad_norm': 7.321716785430908, 'learning_rate': 3.529411764705883e-05, 'epoch': 2.53}                                                                                                   
{'loss': 0.3255, 'grad_norm': 6.872689723968506, 'learning_rate': 2.2222222222222223e-05, 'epoch': 2.73}                                                                                                  
{'loss': 0.3424, 'grad_norm': 7.409019470214844, 'learning_rate': 9.150326797385621e-06, 'epoch': 2.92}                                                                                                   
{'train_runtime': 190.0429, 'train_samples_per_second': 6.488, 'train_steps_per_second': 0.805, 'train_loss': 0.6798230339499081, 'epoch': 2.98}   


1차 추론
max_new_tokens=100 temperature=0.7 top_p=0.9 do_sample=True
[테스트 1] 첫 방문 전사
생성된 인사말: 안녕하세요. 처음 방문인데, 초보 전사에게 적합한 검을 빌려주시겠어요?

[테스트 2] 단골 궁수 (활로 대성공)
생성된 인사말: 사장님~ 저번 활 진짜 대박이었어요! 화살이 다 빗나가지 않고 다 맞았어요!

[테스트 3] 야성형 전사 (도끼로 실패)
생성된 인사말: 크흠! 저번 도끼는 날이 너무 무디더군. 제대로 된 걸로 줘!

2차 추론
max_new_tokens=50 do_sample=False
[테스트 1] 첫 방문 전사
생성된 인사말: 안녕하세요. 처음 뵙겠습니다. 초보 전사인데, 간단한 검을 빌려주실 수 있을까요?

[테스트 2] 단골 궁수 (활로 대성공)
생성된 인사말: 와~ 저번 활 덕분에 몬스터 머리에 구멍 뚫었어! 정말 완벽했어요!

[테스트 3] 야성형 전사 (도끼로 실패)
생성된 인사말: 크하하! 저번 도끼가 좀 무겁더군. 이번엔 가벼운 걸로 줘!


4차 학습
Lora confige의 target_modules를 기존 q_proj, k_proj, v_proj, o_proj들로 이루어진 어텐션 레이어(말투/톤 학습)에다가 
gate_proj, up_proj, down_proj로 이루어진 MLP 레이어(지식/논리 연결 학습)를 추가
{'loss': 3.6577, 'grad_norm': 22.44171714782715, 'learning_rate': 0.00018823529411764707, 'epoch': 0.19}
{'loss': 1.2977, 'grad_norm': 19.55743408203125, 'learning_rate': 0.00017516339869281047, 'epoch': 0.39}                                                                                                  
{'loss': 0.6714, 'grad_norm': 9.417969703674316, 'learning_rate': 0.00016209150326797388, 'epoch': 0.58}                                                                                                  
{'loss': 0.5435, 'grad_norm': 12.009344100952148, 'learning_rate': 0.00014901960784313728, 'epoch': 0.78}                                                                                                 
{'loss': 0.4876, 'grad_norm': 8.931075096130371, 'learning_rate': 0.00013594771241830065, 'epoch': 0.97}                                                                                                  
{'loss': 0.4443, 'grad_norm': 7.660183906555176, 'learning_rate': 0.00012549019607843137, 'epoch': 1.17}                                                                                                  
{'loss': 0.3941, 'grad_norm': 7.379851818084717, 'learning_rate': 0.00011241830065359477, 'epoch': 1.36}                                                                                                  
{'loss': 0.4048, 'grad_norm': 7.199581623077393, 'learning_rate': 9.934640522875818e-05, 'epoch': 1.56}                                                                                                   
{'loss': 0.3865, 'grad_norm': 6.848649024963379, 'learning_rate': 8.627450980392158e-05, 'epoch': 1.75}                                                                                                   
{'loss': 0.3654, 'grad_norm': 6.512887954711914, 'learning_rate': 7.320261437908497e-05, 'epoch': 1.95}                                                                                                   
{'loss': 0.3292, 'grad_norm': 6.3943071365356445, 'learning_rate': 6.0130718954248365e-05, 'epoch': 2.14}                                                                                                 
{'loss': 0.3349, 'grad_norm': 6.925538539886475, 'learning_rate': 4.705882352941177e-05, 'epoch': 2.34}                                                                                                   
{'loss': 0.3113, 'grad_norm': 7.596072673797607, 'learning_rate': 3.3986928104575163e-05, 'epoch': 2.53}                                                                                                  
{'loss': 0.3231, 'grad_norm': 7.184753894805908, 'learning_rate': 2.0915032679738563e-05, 'epoch': 2.73}                                                                                                  
{'loss': 0.3435, 'grad_norm': 7.442220687866211, 'learning_rate': 7.84313725490196e-06, 'epoch': 2.92} 


1차 추론
max_new_tokens=100 temperature=0.7 top_p=0.9 do_sample=True
[테스트 1] 첫 방문 전사
생성된 인사말: 처음 방문합니다. 초보자용 무기 추천 부탁드립니다.

[테스트 2] 단골 궁수 (활로 대성공)
생성된 인사말: 사장님~ 저번 활 덕분에 사냥감 잡았어요! 완전 시원하게!

[테스트 3] 야성형 전사 (도끼로 실패)
생성된 인사말: 크하하! 저번 도끼 날이 너무 무디더군. 날이 sharp한 거 없어?

2차 추론
max_new_tokens=50 do_sample=False
[테스트 1] 첫 방문 전사
생성된 인사말: 안녕하세요. 처음 방문인데, 초보자용 검이 있을까요?

[테스트 2] 단골 궁수 (활로 대성공)
생성된 인사말: 사장님~ 저번 활 진짜 대박이었어요! 몬스터가 다 죽었어요!

[테스트 3] 야성형 전사 (도끼로 실패)
생성된 인사말: 크흠! 저번 도끼는 날이 너무 무디더군. 제대로 된 걸로 줘!


5차 학습
loss가 꾸준히 감소했으니, 에폭을 3에서 10으로 증가
{'loss': 3.632, 'grad_norm': 22.85897445678711, 'learning_rate': 0.00019647058823529413, 'epoch': 0.19}
{'loss': 1.2495, 'grad_norm': 18.423015594482422, 'learning_rate': 0.00019254901960784316, 'epoch': 0.39}                                                                                                 
{'loss': 0.657, 'grad_norm': 8.399713516235352, 'learning_rate': 0.00018862745098039216, 'epoch': 0.58}                                                                                                   
{'loss': 0.5346, 'grad_norm': 10.048810005187988, 'learning_rate': 0.0001847058823529412, 'epoch': 0.78}                                                                                                  
{'loss': 0.5299, 'grad_norm': 151.66082763671875, 'learning_rate': 0.00018196078431372548, 'epoch': 0.97}                                                                                                 
{'loss': 0.4253, 'grad_norm': 7.510385990142822, 'learning_rate': 0.0001780392156862745, 'epoch': 1.17}                                                                                                   
{'loss': 0.3872, 'grad_norm': 6.394371032714844, 'learning_rate': 0.00017411764705882354, 'epoch': 1.36}                                                                                                  
{'loss': 0.4, 'grad_norm': 7.0201945304870605, 'learning_rate': 0.00017019607843137254, 'epoch': 1.56}                                                                                                    
{'loss': 0.3791, 'grad_norm': 5.753494739532471, 'learning_rate': 0.00016627450980392157, 'epoch': 1.75}                                                                                                  
{'loss': 0.3564, 'grad_norm': 6.302947044372559, 'learning_rate': 0.0001623529411764706, 'epoch': 1.95}                                                                                                   
{'loss': 0.316, 'grad_norm': 5.843056678771973, 'learning_rate': 0.0001584313725490196, 'epoch': 2.14}                                                                                                    
{'loss': 0.3249, 'grad_norm': 6.424182415008545, 'learning_rate': 0.00015450980392156863, 'epoch': 2.34}                                                                                                  
{'loss': 0.3006, 'grad_norm': 5.972605228424072, 'learning_rate': 0.00015058823529411766, 'epoch': 2.53}                                                                                                  
{'loss': 0.3146, 'grad_norm': 10.947669982910156, 'learning_rate': 0.00014666666666666666, 'epoch': 2.73}                                                                                                 
{'loss': 0.3309, 'grad_norm': 6.6670660972595215, 'learning_rate': 0.0001427450980392157, 'epoch': 2.92}                                                                                                  
{'loss': 0.2814, 'grad_norm': 5.522458076477051, 'learning_rate': 0.00013882352941176472, 'epoch': 3.11}                                                                                                  
{'loss': 0.2748, 'grad_norm': 6.3676910400390625, 'learning_rate': 0.00013490196078431375, 'epoch': 3.31}                                                                                                 
{'loss': 0.2651, 'grad_norm': 6.696939468383789, 'learning_rate': 0.00013098039215686275, 'epoch': 3.5}                                                                                                   
{'loss': 0.274, 'grad_norm': 5.900905132293701, 'learning_rate': 0.00012705882352941175, 'epoch': 3.7}                                                                                                    
{'loss': 0.2834, 'grad_norm': 6.639902114868164, 'learning_rate': 0.0001231372549019608, 'epoch': 3.89}                                                                                                   
{'loss': 0.2499, 'grad_norm': 5.494331359863281, 'learning_rate': 0.00011921568627450981, 'epoch': 4.09}                                                                                                  
{'loss': 0.2338, 'grad_norm': 8.220526695251465, 'learning_rate': 0.00011529411764705881, 'epoch': 4.28}                                                                                                  
{'loss': 0.2489, 'grad_norm': 6.968906402587891, 'learning_rate': 0.00011137254901960786, 'epoch': 4.48}                                                                                                  
{'loss': 0.2355, 'grad_norm': 6.343825817108154, 'learning_rate': 0.00010745098039215686, 'epoch': 4.67}                                                                                                  
{'loss': 0.2472, 'grad_norm': 6.725374698638916, 'learning_rate': 0.0001035294117647059, 'epoch': 4.87}                                                                                                   
{'loss': 0.2369, 'grad_norm': 6.817840099334717, 'learning_rate': 9.96078431372549e-05, 'epoch': 5.06}                                                                                                    
{'loss': 0.2197, 'grad_norm': 8.312772750854492, 'learning_rate': 9.568627450980393e-05, 'epoch': 5.26}                                                                                                   
{'loss': 0.2095, 'grad_norm': 6.425045013427734, 'learning_rate': 9.176470588235295e-05, 'epoch': 5.45}                                                                                                   
{'loss': 0.2064, 'grad_norm': 7.308022499084473, 'learning_rate': 8.784313725490196e-05, 'epoch': 5.64}                                                                                                   
{'loss': 0.2081, 'grad_norm': 8.767561912536621, 'learning_rate': 8.392156862745099e-05, 'epoch': 5.84}                                                                                                   
{'loss': 0.1975, 'grad_norm': 6.444986343383789, 'learning_rate': 8e-05, 'epoch': 6.03}                                                                                                                   
{'loss': 0.1921, 'grad_norm': 9.093132972717285, 'learning_rate': 7.607843137254902e-05, 'epoch': 6.23}                                                                                                   
{'loss': 0.1844, 'grad_norm': 6.256903648376465, 'learning_rate': 7.215686274509804e-05, 'epoch': 6.42}                                                                                                   
{'loss': 0.1847, 'grad_norm': 7.4056572914123535, 'learning_rate': 6.823529411764707e-05, 'epoch': 6.62}                                                                                                  
{'loss': 0.1772, 'grad_norm': 7.957502365112305, 'learning_rate': 6.431372549019608e-05, 'epoch': 6.81}                                                                                                   
{'loss': 0.188, 'grad_norm': 6.845193386077881, 'learning_rate': 6.03921568627451e-05, 'epoch': 7.01}                                                                                                     
{'loss': 0.1619, 'grad_norm': 7.349745273590088, 'learning_rate': 5.647058823529412e-05, 'epoch': 7.2}                                                                                                    
{'loss': 0.1652, 'grad_norm': 7.038292407989502, 'learning_rate': 5.254901960784314e-05, 'epoch': 7.4}                                                                                                    
{'loss': 0.1625, 'grad_norm': 7.7898783683776855, 'learning_rate': 4.862745098039216e-05, 'epoch': 7.59}                                                                                                  
{'loss': 0.16, 'grad_norm': 7.152511119842529, 'learning_rate': 4.470588235294118e-05, 'epoch': 7.79}                                                                                                     
{'loss': 0.1644, 'grad_norm': 7.99098014831543, 'learning_rate': 4.0784313725490195e-05, 'epoch': 7.98}                                                                                                   
{'loss': 0.1571, 'grad_norm': 6.214821815490723, 'learning_rate': 3.686274509803922e-05, 'epoch': 8.18}                                                                                                   
{'loss': 0.1406, 'grad_norm': 7.496259689331055, 'learning_rate': 3.294117647058824e-05, 'epoch': 8.37}                                                                                                   
{'loss': 0.1476, 'grad_norm': 8.457307815551758, 'learning_rate': 2.9019607843137258e-05, 'epoch': 8.56}                                                                                                  
{'loss': 0.145, 'grad_norm': 6.238116264343262, 'learning_rate': 2.5098039215686277e-05, 'epoch': 8.76}                                                                                                   
{'loss': 0.1482, 'grad_norm': 7.148946762084961, 'learning_rate': 2.1176470588235296e-05, 'epoch': 8.95}                                                                                                  
{'loss': 0.1344, 'grad_norm': 5.65740966796875, 'learning_rate': 1.7254901960784314e-05, 'epoch': 9.15}                                                                                                   
{'loss': 0.1296, 'grad_norm': 6.902994632720947, 'learning_rate': 1.3333333333333333e-05, 'epoch': 9.34}                                                                                                  
{'loss': 0.136, 'grad_norm': 5.825518608093262, 'learning_rate': 9.411764705882354e-06, 'epoch': 9.54}                                                                                                    
{'loss': 0.141, 'grad_norm': 6.815798282623291, 'learning_rate': 5.490196078431373e-06, 'epoch': 9.73}                                                                                                    
{'loss': 0.13, 'grad_norm': 6.3079142570495605, 'learning_rate': 1.5686274509803923e-06, 'epoch': 9.93}  

1차 추론
max_new_tokens=50 do_sample=False
[테스트 1] 첫 방문 전사
생성된 인사말: 처음 방문합니다. 초보 전사용 검 있을까요?

[테스트 2] 단골 궁수 (활로 대성공)
생성된 인사말: 사장님! 저번 활 대박이었어요! 다 맞춘 거 있죠?

[테스트 3] 야성형 전사 (도끼로 실패)
생성된 인사말: 크흠! 저번 도끼 날이 무뎠어. 튼튼한 놈으로 다시 줘!