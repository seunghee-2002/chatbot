{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 9.976171564733916,
  "eval_steps": 500,
  "global_step": 1570,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.06354249404289118,
      "grad_norm": 23.593555450439453,
      "learning_rate": 0.00019885350318471338,
      "loss": 3.6793,
      "step": 10
    },
    {
      "epoch": 0.12708498808578236,
      "grad_norm": 18.63372802734375,
      "learning_rate": 0.00019757961783439492,
      "loss": 1.2601,
      "step": 20
    },
    {
      "epoch": 0.19062748212867356,
      "grad_norm": 13.890305519104004,
      "learning_rate": 0.00019630573248407643,
      "loss": 0.6752,
      "step": 30
    },
    {
      "epoch": 0.2541699761715647,
      "grad_norm": 8.420939445495605,
      "learning_rate": 0.00019503184713375797,
      "loss": 0.5734,
      "step": 40
    },
    {
      "epoch": 0.3177124702144559,
      "grad_norm": 7.317549705505371,
      "learning_rate": 0.00019414012738853503,
      "loss": 0.5097,
      "step": 50
    },
    {
      "epoch": 0.3812549642573471,
      "grad_norm": 6.396265029907227,
      "learning_rate": 0.00019286624203821657,
      "loss": 0.4572,
      "step": 60
    },
    {
      "epoch": 0.44479745830023826,
      "grad_norm": 6.5263895988464355,
      "learning_rate": 0.0001915923566878981,
      "loss": 0.4522,
      "step": 70
    },
    {
      "epoch": 0.5083399523431295,
      "grad_norm": 6.206676006317139,
      "learning_rate": 0.00019031847133757962,
      "loss": 0.419,
      "step": 80
    },
    {
      "epoch": 0.5718824463860207,
      "grad_norm": 5.469913005828857,
      "learning_rate": 0.00018904458598726116,
      "loss": 0.4276,
      "step": 90
    },
    {
      "epoch": 0.6354249404289118,
      "grad_norm": 5.732343673706055,
      "learning_rate": 0.0001877707006369427,
      "loss": 0.4267,
      "step": 100
    },
    {
      "epoch": 0.698967434471803,
      "grad_norm": 6.354297637939453,
      "learning_rate": 0.0001864968152866242,
      "loss": 0.376,
      "step": 110
    },
    {
      "epoch": 0.7625099285146942,
      "grad_norm": 5.4953999519348145,
      "learning_rate": 0.00018522292993630575,
      "loss": 0.3963,
      "step": 120
    },
    {
      "epoch": 0.8260524225575854,
      "grad_norm": 6.290401935577393,
      "learning_rate": 0.00018394904458598728,
      "loss": 0.3933,
      "step": 130
    },
    {
      "epoch": 0.8895949166004765,
      "grad_norm": 5.417150020599365,
      "learning_rate": 0.0001826751592356688,
      "loss": 0.3813,
      "step": 140
    },
    {
      "epoch": 0.9531374106433678,
      "grad_norm": 4.567422389984131,
      "learning_rate": 0.00018140127388535033,
      "loss": 0.3944,
      "step": 150
    },
    {
      "epoch": 1.016679904686259,
      "grad_norm": 5.116743564605713,
      "learning_rate": 0.00018012738853503187,
      "loss": 0.3763,
      "step": 160
    },
    {
      "epoch": 1.0802223987291502,
      "grad_norm": 4.42767858505249,
      "learning_rate": 0.00017885350318471338,
      "loss": 0.3289,
      "step": 170
    },
    {
      "epoch": 1.1437648927720412,
      "grad_norm": 5.030894756317139,
      "learning_rate": 0.00017757961783439492,
      "loss": 0.3547,
      "step": 180
    },
    {
      "epoch": 1.2073073868149324,
      "grad_norm": 5.538336753845215,
      "learning_rate": 0.00017630573248407646,
      "loss": 0.3122,
      "step": 190
    },
    {
      "epoch": 1.2708498808578237,
      "grad_norm": 5.414416313171387,
      "learning_rate": 0.00017503184713375797,
      "loss": 0.3359,
      "step": 200
    },
    {
      "epoch": 1.3343923749007147,
      "grad_norm": 5.167139530181885,
      "learning_rate": 0.0001737579617834395,
      "loss": 0.3357,
      "step": 210
    },
    {
      "epoch": 1.397934868943606,
      "grad_norm": 5.400119304656982,
      "learning_rate": 0.00017248407643312105,
      "loss": 0.3429,
      "step": 220
    },
    {
      "epoch": 1.4614773629864972,
      "grad_norm": 4.901339054107666,
      "learning_rate": 0.00017121019108280256,
      "loss": 0.3378,
      "step": 230
    },
    {
      "epoch": 1.5250198570293882,
      "grad_norm": 4.979988098144531,
      "learning_rate": 0.0001699363057324841,
      "loss": 0.3455,
      "step": 240
    },
    {
      "epoch": 1.5885623510722797,
      "grad_norm": 6.194584369659424,
      "learning_rate": 0.0001686624203821656,
      "loss": 0.3411,
      "step": 250
    },
    {
      "epoch": 1.6521048451151708,
      "grad_norm": 4.959116458892822,
      "learning_rate": 0.00016738853503184715,
      "loss": 0.3515,
      "step": 260
    },
    {
      "epoch": 1.715647339158062,
      "grad_norm": 5.596278190612793,
      "learning_rate": 0.00016611464968152868,
      "loss": 0.3365,
      "step": 270
    },
    {
      "epoch": 1.7791898332009533,
      "grad_norm": 4.726241588592529,
      "learning_rate": 0.0001648407643312102,
      "loss": 0.346,
      "step": 280
    },
    {
      "epoch": 1.8427323272438443,
      "grad_norm": 5.179662227630615,
      "learning_rate": 0.00016356687898089173,
      "loss": 0.3405,
      "step": 290
    },
    {
      "epoch": 1.9062748212867355,
      "grad_norm": 4.70369815826416,
      "learning_rate": 0.00016229299363057327,
      "loss": 0.3172,
      "step": 300
    },
    {
      "epoch": 1.9698173153296268,
      "grad_norm": 5.569770812988281,
      "learning_rate": 0.00016101910828025478,
      "loss": 0.3199,
      "step": 310
    },
    {
      "epoch": 2.033359809372518,
      "grad_norm": 4.972203254699707,
      "learning_rate": 0.00015974522292993632,
      "loss": 0.3071,
      "step": 320
    },
    {
      "epoch": 2.096902303415409,
      "grad_norm": 4.807532787322998,
      "learning_rate": 0.00015847133757961786,
      "loss": 0.2911,
      "step": 330
    },
    {
      "epoch": 2.1604447974583003,
      "grad_norm": 5.4726643562316895,
      "learning_rate": 0.00015719745222929937,
      "loss": 0.2918,
      "step": 340
    },
    {
      "epoch": 2.2239872915011913,
      "grad_norm": 5.166556358337402,
      "learning_rate": 0.0001559235668789809,
      "loss": 0.2839,
      "step": 350
    },
    {
      "epoch": 2.2875297855440824,
      "grad_norm": 5.173545837402344,
      "learning_rate": 0.00015464968152866245,
      "loss": 0.2799,
      "step": 360
    },
    {
      "epoch": 2.351072279586974,
      "grad_norm": 5.869082927703857,
      "learning_rate": 0.00015337579617834396,
      "loss": 0.2875,
      "step": 370
    },
    {
      "epoch": 2.414614773629865,
      "grad_norm": 5.290470123291016,
      "learning_rate": 0.0001521019108280255,
      "loss": 0.278,
      "step": 380
    },
    {
      "epoch": 2.4781572676727563,
      "grad_norm": 5.282162666320801,
      "learning_rate": 0.00015082802547770703,
      "loss": 0.3014,
      "step": 390
    },
    {
      "epoch": 2.5416997617156474,
      "grad_norm": 4.748305320739746,
      "learning_rate": 0.00014955414012738852,
      "loss": 0.2697,
      "step": 400
    },
    {
      "epoch": 2.6052422557585384,
      "grad_norm": 4.880847930908203,
      "learning_rate": 0.00014828025477707006,
      "loss": 0.277,
      "step": 410
    },
    {
      "epoch": 2.6687847498014294,
      "grad_norm": 6.057268142700195,
      "learning_rate": 0.0001470063694267516,
      "loss": 0.2952,
      "step": 420
    },
    {
      "epoch": 2.732327243844321,
      "grad_norm": 5.28296422958374,
      "learning_rate": 0.0001457324840764331,
      "loss": 0.2772,
      "step": 430
    },
    {
      "epoch": 2.795869737887212,
      "grad_norm": 5.285755157470703,
      "learning_rate": 0.00014445859872611464,
      "loss": 0.2726,
      "step": 440
    },
    {
      "epoch": 2.8594122319301034,
      "grad_norm": 5.365350246429443,
      "learning_rate": 0.00014318471337579618,
      "loss": 0.2765,
      "step": 450
    },
    {
      "epoch": 2.9229547259729944,
      "grad_norm": 5.375385761260986,
      "learning_rate": 0.0001419108280254777,
      "loss": 0.2838,
      "step": 460
    },
    {
      "epoch": 2.9864972200158855,
      "grad_norm": 5.659815311431885,
      "learning_rate": 0.00014063694267515923,
      "loss": 0.2741,
      "step": 470
    },
    {
      "epoch": 3.050039714058777,
      "grad_norm": 4.668781280517578,
      "learning_rate": 0.00013936305732484077,
      "loss": 0.2551,
      "step": 480
    },
    {
      "epoch": 3.113582208101668,
      "grad_norm": 6.971310615539551,
      "learning_rate": 0.00013808917197452228,
      "loss": 0.2316,
      "step": 490
    },
    {
      "epoch": 3.177124702144559,
      "grad_norm": 5.877250671386719,
      "learning_rate": 0.00013681528662420382,
      "loss": 0.2399,
      "step": 500
    },
    {
      "epoch": 3.2406671961874505,
      "grad_norm": 5.391688346862793,
      "learning_rate": 0.00013554140127388536,
      "loss": 0.231,
      "step": 510
    },
    {
      "epoch": 3.3042096902303415,
      "grad_norm": 5.072035789489746,
      "learning_rate": 0.00013426751592356687,
      "loss": 0.2396,
      "step": 520
    },
    {
      "epoch": 3.3677521842732325,
      "grad_norm": 5.196191310882568,
      "learning_rate": 0.0001329936305732484,
      "loss": 0.2233,
      "step": 530
    },
    {
      "epoch": 3.431294678316124,
      "grad_norm": 5.419106960296631,
      "learning_rate": 0.00013171974522292995,
      "loss": 0.2347,
      "step": 540
    },
    {
      "epoch": 3.494837172359015,
      "grad_norm": 5.730679035186768,
      "learning_rate": 0.00013044585987261146,
      "loss": 0.2465,
      "step": 550
    },
    {
      "epoch": 3.5583796664019065,
      "grad_norm": 6.366321086883545,
      "learning_rate": 0.000129171974522293,
      "loss": 0.2332,
      "step": 560
    },
    {
      "epoch": 3.6219221604447975,
      "grad_norm": 5.758443355560303,
      "learning_rate": 0.00012789808917197453,
      "loss": 0.228,
      "step": 570
    },
    {
      "epoch": 3.6854646544876886,
      "grad_norm": 5.704713821411133,
      "learning_rate": 0.00012662420382165604,
      "loss": 0.2446,
      "step": 580
    },
    {
      "epoch": 3.7490071485305796,
      "grad_norm": 5.432254791259766,
      "learning_rate": 0.00012535031847133758,
      "loss": 0.2452,
      "step": 590
    },
    {
      "epoch": 3.812549642573471,
      "grad_norm": 6.247969627380371,
      "learning_rate": 0.00012407643312101912,
      "loss": 0.2405,
      "step": 600
    },
    {
      "epoch": 3.876092136616362,
      "grad_norm": 5.928271293640137,
      "learning_rate": 0.00012280254777070063,
      "loss": 0.2385,
      "step": 610
    },
    {
      "epoch": 3.9396346306592536,
      "grad_norm": 6.023488521575928,
      "learning_rate": 0.00012152866242038217,
      "loss": 0.2448,
      "step": 620
    },
    {
      "epoch": 4.003177124702145,
      "grad_norm": 5.736695766448975,
      "learning_rate": 0.0001202547770700637,
      "loss": 0.2341,
      "step": 630
    },
    {
      "epoch": 4.066719618745036,
      "grad_norm": 5.2283549308776855,
      "learning_rate": 0.00011898089171974522,
      "loss": 0.1984,
      "step": 640
    },
    {
      "epoch": 4.130262112787927,
      "grad_norm": 5.152540683746338,
      "learning_rate": 0.00011770700636942676,
      "loss": 0.1883,
      "step": 650
    },
    {
      "epoch": 4.193804606830818,
      "grad_norm": 6.60544490814209,
      "learning_rate": 0.00011643312101910828,
      "loss": 0.2007,
      "step": 660
    },
    {
      "epoch": 4.25734710087371,
      "grad_norm": 5.562497615814209,
      "learning_rate": 0.00011515923566878981,
      "loss": 0.1962,
      "step": 670
    },
    {
      "epoch": 4.320889594916601,
      "grad_norm": 6.2935991287231445,
      "learning_rate": 0.00011388535031847135,
      "loss": 0.1966,
      "step": 680
    },
    {
      "epoch": 4.384432088959492,
      "grad_norm": 6.0112223625183105,
      "learning_rate": 0.00011261146496815287,
      "loss": 0.2072,
      "step": 690
    },
    {
      "epoch": 4.447974583002383,
      "grad_norm": 5.803299427032471,
      "learning_rate": 0.0001113375796178344,
      "loss": 0.2032,
      "step": 700
    },
    {
      "epoch": 4.511517077045274,
      "grad_norm": 6.461359977722168,
      "learning_rate": 0.00011006369426751592,
      "loss": 0.195,
      "step": 710
    },
    {
      "epoch": 4.575059571088165,
      "grad_norm": 5.713562488555908,
      "learning_rate": 0.00010878980891719746,
      "loss": 0.2053,
      "step": 720
    },
    {
      "epoch": 4.638602065131057,
      "grad_norm": 6.399006366729736,
      "learning_rate": 0.00010751592356687898,
      "loss": 0.2006,
      "step": 730
    },
    {
      "epoch": 4.702144559173948,
      "grad_norm": 5.438851356506348,
      "learning_rate": 0.00010624203821656051,
      "loss": 0.2062,
      "step": 740
    },
    {
      "epoch": 4.765687053216839,
      "grad_norm": 5.954084873199463,
      "learning_rate": 0.00010496815286624205,
      "loss": 0.2027,
      "step": 750
    },
    {
      "epoch": 4.82922954725973,
      "grad_norm": 7.603062152862549,
      "learning_rate": 0.00010369426751592357,
      "loss": 0.2036,
      "step": 760
    },
    {
      "epoch": 4.892772041302621,
      "grad_norm": 6.2848711013793945,
      "learning_rate": 0.0001024203821656051,
      "loss": 0.2009,
      "step": 770
    },
    {
      "epoch": 4.956314535345513,
      "grad_norm": 5.6236748695373535,
      "learning_rate": 0.00010114649681528663,
      "loss": 0.2037,
      "step": 780
    },
    {
      "epoch": 5.019857029388404,
      "grad_norm": 4.513431549072266,
      "learning_rate": 9.987261146496816e-05,
      "loss": 0.1973,
      "step": 790
    },
    {
      "epoch": 5.083399523431295,
      "grad_norm": 5.102558135986328,
      "learning_rate": 9.859872611464968e-05,
      "loss": 0.162,
      "step": 800
    },
    {
      "epoch": 5.146942017474186,
      "grad_norm": 6.7467041015625,
      "learning_rate": 9.732484076433122e-05,
      "loss": 0.1748,
      "step": 810
    },
    {
      "epoch": 5.210484511517077,
      "grad_norm": 5.2051825523376465,
      "learning_rate": 9.605095541401275e-05,
      "loss": 0.1701,
      "step": 820
    },
    {
      "epoch": 5.274027005559968,
      "grad_norm": 5.201944351196289,
      "learning_rate": 9.477707006369427e-05,
      "loss": 0.1651,
      "step": 830
    },
    {
      "epoch": 5.33756949960286,
      "grad_norm": 4.674276828765869,
      "learning_rate": 9.35031847133758e-05,
      "loss": 0.1715,
      "step": 840
    },
    {
      "epoch": 5.401111993645751,
      "grad_norm": 6.54340934753418,
      "learning_rate": 9.222929936305733e-05,
      "loss": 0.1745,
      "step": 850
    },
    {
      "epoch": 5.464654487688642,
      "grad_norm": 6.419454097747803,
      "learning_rate": 9.095541401273886e-05,
      "loss": 0.1752,
      "step": 860
    },
    {
      "epoch": 5.528196981731533,
      "grad_norm": 5.762249946594238,
      "learning_rate": 8.968152866242038e-05,
      "loss": 0.176,
      "step": 870
    },
    {
      "epoch": 5.591739475774424,
      "grad_norm": 5.985104084014893,
      "learning_rate": 8.840764331210192e-05,
      "loss": 0.1739,
      "step": 880
    },
    {
      "epoch": 5.655281969817315,
      "grad_norm": 6.405752658843994,
      "learning_rate": 8.713375796178345e-05,
      "loss": 0.1753,
      "step": 890
    },
    {
      "epoch": 5.718824463860207,
      "grad_norm": 7.125116348266602,
      "learning_rate": 8.585987261146497e-05,
      "loss": 0.1741,
      "step": 900
    },
    {
      "epoch": 5.782366957903098,
      "grad_norm": 5.7283501625061035,
      "learning_rate": 8.458598726114651e-05,
      "loss": 0.1751,
      "step": 910
    },
    {
      "epoch": 5.845909451945989,
      "grad_norm": 6.248725414276123,
      "learning_rate": 8.331210191082803e-05,
      "loss": 0.1842,
      "step": 920
    },
    {
      "epoch": 5.90945194598888,
      "grad_norm": 5.162700653076172,
      "learning_rate": 8.203821656050956e-05,
      "loss": 0.1734,
      "step": 930
    },
    {
      "epoch": 5.972994440031771,
      "grad_norm": 5.082780361175537,
      "learning_rate": 8.07643312101911e-05,
      "loss": 0.1697,
      "step": 940
    },
    {
      "epoch": 6.036536934074663,
      "grad_norm": 5.047996520996094,
      "learning_rate": 7.949044585987262e-05,
      "loss": 0.1565,
      "step": 950
    },
    {
      "epoch": 6.100079428117554,
      "grad_norm": 6.258172988891602,
      "learning_rate": 7.821656050955415e-05,
      "loss": 0.1436,
      "step": 960
    },
    {
      "epoch": 6.163621922160445,
      "grad_norm": 5.231616497039795,
      "learning_rate": 7.694267515923567e-05,
      "loss": 0.143,
      "step": 970
    },
    {
      "epoch": 6.227164416203336,
      "grad_norm": 6.014667987823486,
      "learning_rate": 7.566878980891721e-05,
      "loss": 0.1443,
      "step": 980
    },
    {
      "epoch": 6.290706910246227,
      "grad_norm": 5.709659099578857,
      "learning_rate": 7.439490445859872e-05,
      "loss": 0.1458,
      "step": 990
    },
    {
      "epoch": 6.354249404289118,
      "grad_norm": 4.739134788513184,
      "learning_rate": 7.312101910828026e-05,
      "loss": 0.148,
      "step": 1000
    },
    {
      "epoch": 6.41779189833201,
      "grad_norm": 5.887192249298096,
      "learning_rate": 7.184713375796178e-05,
      "loss": 0.1507,
      "step": 1010
    },
    {
      "epoch": 6.481334392374901,
      "grad_norm": 5.972622871398926,
      "learning_rate": 7.057324840764331e-05,
      "loss": 0.1576,
      "step": 1020
    },
    {
      "epoch": 6.544876886417792,
      "grad_norm": 6.1182050704956055,
      "learning_rate": 6.929936305732483e-05,
      "loss": 0.1508,
      "step": 1030
    },
    {
      "epoch": 6.608419380460683,
      "grad_norm": 6.727288722991943,
      "learning_rate": 6.802547770700637e-05,
      "loss": 0.1517,
      "step": 1040
    },
    {
      "epoch": 6.671961874503574,
      "grad_norm": 8.186197280883789,
      "learning_rate": 6.67515923566879e-05,
      "loss": 0.1551,
      "step": 1050
    },
    {
      "epoch": 6.735504368546465,
      "grad_norm": 5.203182220458984,
      "learning_rate": 6.547770700636942e-05,
      "loss": 0.1458,
      "step": 1060
    },
    {
      "epoch": 6.799046862589357,
      "grad_norm": 4.893793106079102,
      "learning_rate": 6.420382165605096e-05,
      "loss": 0.1623,
      "step": 1070
    },
    {
      "epoch": 6.862589356632248,
      "grad_norm": 7.265105247497559,
      "learning_rate": 6.292993630573248e-05,
      "loss": 0.1463,
      "step": 1080
    },
    {
      "epoch": 6.926131850675139,
      "grad_norm": 6.583233833312988,
      "learning_rate": 6.165605095541401e-05,
      "loss": 0.1586,
      "step": 1090
    },
    {
      "epoch": 6.98967434471803,
      "grad_norm": 4.851688385009766,
      "learning_rate": 6.038216560509554e-05,
      "loss": 0.1573,
      "step": 1100
    },
    {
      "epoch": 7.053216838760921,
      "grad_norm": 5.11968469619751,
      "learning_rate": 5.910828025477707e-05,
      "loss": 0.1267,
      "step": 1110
    },
    {
      "epoch": 7.116759332803812,
      "grad_norm": 5.362842082977295,
      "learning_rate": 5.78343949044586e-05,
      "loss": 0.1278,
      "step": 1120
    },
    {
      "epoch": 7.180301826846704,
      "grad_norm": 5.586505889892578,
      "learning_rate": 5.656050955414013e-05,
      "loss": 0.1309,
      "step": 1130
    },
    {
      "epoch": 7.243844320889595,
      "grad_norm": 4.923676490783691,
      "learning_rate": 5.528662420382166e-05,
      "loss": 0.1334,
      "step": 1140
    },
    {
      "epoch": 7.307386814932486,
      "grad_norm": 6.1035614013671875,
      "learning_rate": 5.4012738853503184e-05,
      "loss": 0.1331,
      "step": 1150
    },
    {
      "epoch": 7.370929308975377,
      "grad_norm": 5.370506286621094,
      "learning_rate": 5.2738853503184715e-05,
      "loss": 0.1334,
      "step": 1160
    },
    {
      "epoch": 7.434471803018268,
      "grad_norm": 6.123373508453369,
      "learning_rate": 5.146496815286625e-05,
      "loss": 0.1356,
      "step": 1170
    },
    {
      "epoch": 7.498014297061159,
      "grad_norm": 5.865098476409912,
      "learning_rate": 5.019108280254777e-05,
      "loss": 0.1346,
      "step": 1180
    },
    {
      "epoch": 7.561556791104051,
      "grad_norm": 5.589290618896484,
      "learning_rate": 4.89171974522293e-05,
      "loss": 0.1372,
      "step": 1190
    },
    {
      "epoch": 7.625099285146942,
      "grad_norm": 4.31352424621582,
      "learning_rate": 4.764331210191083e-05,
      "loss": 0.132,
      "step": 1200
    },
    {
      "epoch": 7.688641779189833,
      "grad_norm": 6.201558589935303,
      "learning_rate": 4.636942675159236e-05,
      "loss": 0.1356,
      "step": 1210
    },
    {
      "epoch": 7.752184273232724,
      "grad_norm": 4.871872901916504,
      "learning_rate": 4.509554140127389e-05,
      "loss": 0.1278,
      "step": 1220
    },
    {
      "epoch": 7.815726767275615,
      "grad_norm": 4.658520221710205,
      "learning_rate": 4.3821656050955416e-05,
      "loss": 0.1362,
      "step": 1230
    },
    {
      "epoch": 7.879269261318507,
      "grad_norm": 5.719579219818115,
      "learning_rate": 4.254777070063695e-05,
      "loss": 0.1417,
      "step": 1240
    },
    {
      "epoch": 7.942811755361398,
      "grad_norm": 5.108421802520752,
      "learning_rate": 4.127388535031848e-05,
      "loss": 0.1346,
      "step": 1250
    },
    {
      "epoch": 8.00635424940429,
      "grad_norm": 3.7922258377075195,
      "learning_rate": 4e-05,
      "loss": 0.1326,
      "step": 1260
    },
    {
      "epoch": 8.069896743447181,
      "grad_norm": 4.500579833984375,
      "learning_rate": 3.8726114649681535e-05,
      "loss": 0.1177,
      "step": 1270
    },
    {
      "epoch": 8.133439237490071,
      "grad_norm": 6.074751853942871,
      "learning_rate": 3.745222929936306e-05,
      "loss": 0.1218,
      "step": 1280
    },
    {
      "epoch": 8.196981731532963,
      "grad_norm": 4.513510227203369,
      "learning_rate": 3.6178343949044584e-05,
      "loss": 0.1193,
      "step": 1290
    },
    {
      "epoch": 8.260524225575853,
      "grad_norm": 4.435558795928955,
      "learning_rate": 3.4904458598726116e-05,
      "loss": 0.12,
      "step": 1300
    },
    {
      "epoch": 8.324066719618745,
      "grad_norm": 5.520767688751221,
      "learning_rate": 3.363057324840764e-05,
      "loss": 0.1222,
      "step": 1310
    },
    {
      "epoch": 8.387609213661635,
      "grad_norm": 4.899604320526123,
      "learning_rate": 3.235668789808917e-05,
      "loss": 0.1231,
      "step": 1320
    },
    {
      "epoch": 8.451151707704527,
      "grad_norm": 4.299341201782227,
      "learning_rate": 3.1082802547770703e-05,
      "loss": 0.1169,
      "step": 1330
    },
    {
      "epoch": 8.51469420174742,
      "grad_norm": 4.750171661376953,
      "learning_rate": 2.9808917197452228e-05,
      "loss": 0.1188,
      "step": 1340
    },
    {
      "epoch": 8.57823669579031,
      "grad_norm": 3.9720571041107178,
      "learning_rate": 2.853503184713376e-05,
      "loss": 0.1244,
      "step": 1350
    },
    {
      "epoch": 8.641779189833201,
      "grad_norm": 3.9795114994049072,
      "learning_rate": 2.7261146496815288e-05,
      "loss": 0.1122,
      "step": 1360
    },
    {
      "epoch": 8.705321683876091,
      "grad_norm": 5.193479537963867,
      "learning_rate": 2.5987261146496816e-05,
      "loss": 0.1285,
      "step": 1370
    },
    {
      "epoch": 8.768864177918983,
      "grad_norm": 4.345576286315918,
      "learning_rate": 2.4713375796178344e-05,
      "loss": 0.1172,
      "step": 1380
    },
    {
      "epoch": 8.832406671961875,
      "grad_norm": 6.045894145965576,
      "learning_rate": 2.3439490445859875e-05,
      "loss": 0.1217,
      "step": 1390
    },
    {
      "epoch": 8.895949166004765,
      "grad_norm": 4.610732555389404,
      "learning_rate": 2.2165605095541404e-05,
      "loss": 0.1191,
      "step": 1400
    },
    {
      "epoch": 8.959491660047657,
      "grad_norm": 5.460098743438721,
      "learning_rate": 2.089171974522293e-05,
      "loss": 0.1273,
      "step": 1410
    },
    {
      "epoch": 9.023034154090547,
      "grad_norm": 4.500016689300537,
      "learning_rate": 1.961783439490446e-05,
      "loss": 0.1196,
      "step": 1420
    },
    {
      "epoch": 9.08657664813344,
      "grad_norm": 4.2286376953125,
      "learning_rate": 1.8343949044585988e-05,
      "loss": 0.1073,
      "step": 1430
    },
    {
      "epoch": 9.150119142176331,
      "grad_norm": 5.329665184020996,
      "learning_rate": 1.7070063694267516e-05,
      "loss": 0.1096,
      "step": 1440
    },
    {
      "epoch": 9.213661636219221,
      "grad_norm": 5.880336761474609,
      "learning_rate": 1.5796178343949044e-05,
      "loss": 0.1134,
      "step": 1450
    },
    {
      "epoch": 9.277204130262113,
      "grad_norm": 4.403232574462891,
      "learning_rate": 1.4522292993630574e-05,
      "loss": 0.107,
      "step": 1460
    },
    {
      "epoch": 9.340746624305003,
      "grad_norm": 3.8063998222351074,
      "learning_rate": 1.3248407643312102e-05,
      "loss": 0.1117,
      "step": 1470
    },
    {
      "epoch": 9.404289118347895,
      "grad_norm": 5.236371040344238,
      "learning_rate": 1.1974522292993632e-05,
      "loss": 0.1108,
      "step": 1480
    },
    {
      "epoch": 9.467831612390786,
      "grad_norm": 5.770538330078125,
      "learning_rate": 1.070063694267516e-05,
      "loss": 0.1143,
      "step": 1490
    },
    {
      "epoch": 9.531374106433677,
      "grad_norm": 4.279350280761719,
      "learning_rate": 9.42675159235669e-06,
      "loss": 0.1079,
      "step": 1500
    },
    {
      "epoch": 9.59491660047657,
      "grad_norm": 4.7367424964904785,
      "learning_rate": 8.152866242038216e-06,
      "loss": 0.1092,
      "step": 1510
    },
    {
      "epoch": 9.65845909451946,
      "grad_norm": 5.621295928955078,
      "learning_rate": 6.878980891719745e-06,
      "loss": 0.1103,
      "step": 1520
    },
    {
      "epoch": 9.722001588562351,
      "grad_norm": 4.625283718109131,
      "learning_rate": 5.605095541401274e-06,
      "loss": 0.1175,
      "step": 1530
    },
    {
      "epoch": 9.785544082605242,
      "grad_norm": 5.1069512367248535,
      "learning_rate": 4.331210191082802e-06,
      "loss": 0.113,
      "step": 1540
    },
    {
      "epoch": 9.849086576648133,
      "grad_norm": 5.745232105255127,
      "learning_rate": 3.0573248407643314e-06,
      "loss": 0.1073,
      "step": 1550
    },
    {
      "epoch": 9.912629070691025,
      "grad_norm": 4.115090370178223,
      "learning_rate": 1.78343949044586e-06,
      "loss": 0.1139,
      "step": 1560
    },
    {
      "epoch": 9.976171564733916,
      "grad_norm": 3.9579014778137207,
      "learning_rate": 5.095541401273885e-07,
      "loss": 0.1105,
      "step": 1570
    }
  ],
  "logging_steps": 10,
  "max_steps": 1570,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.137167171772416e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
