{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 7.981859410430839,
  "eval_steps": 500,
  "global_step": 1760,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.045351473922902494,
      "grad_norm": 22.600505828857422,
      "learning_rate": 0.00019897727272727273,
      "loss": 3.6523,
      "step": 10
    },
    {
      "epoch": 0.09070294784580499,
      "grad_norm": 14.163262367248535,
      "learning_rate": 0.00019784090909090912,
      "loss": 1.2907,
      "step": 20
    },
    {
      "epoch": 0.1360544217687075,
      "grad_norm": 9.082428932189941,
      "learning_rate": 0.00019670454545454546,
      "loss": 0.7132,
      "step": 30
    },
    {
      "epoch": 0.18140589569160998,
      "grad_norm": 6.289700508117676,
      "learning_rate": 0.00019556818181818182,
      "loss": 0.5709,
      "step": 40
    },
    {
      "epoch": 0.22675736961451248,
      "grad_norm": 303.3710632324219,
      "learning_rate": 0.00019488636363636366,
      "loss": 0.5812,
      "step": 50
    },
    {
      "epoch": 0.272108843537415,
      "grad_norm": 6.102778434753418,
      "learning_rate": 0.00019375000000000002,
      "loss": 0.5209,
      "step": 60
    },
    {
      "epoch": 0.31746031746031744,
      "grad_norm": 5.914968490600586,
      "learning_rate": 0.00019261363636363635,
      "loss": 0.4582,
      "step": 70
    },
    {
      "epoch": 0.36281179138321995,
      "grad_norm": 6.6005473136901855,
      "learning_rate": 0.00019147727272727274,
      "loss": 0.4821,
      "step": 80
    },
    {
      "epoch": 0.40816326530612246,
      "grad_norm": 5.530953407287598,
      "learning_rate": 0.0001903409090909091,
      "loss": 0.4303,
      "step": 90
    },
    {
      "epoch": 0.45351473922902497,
      "grad_norm": 5.321864604949951,
      "learning_rate": 0.00018920454545454546,
      "loss": 0.4319,
      "step": 100
    },
    {
      "epoch": 0.4988662131519274,
      "grad_norm": 5.429179668426514,
      "learning_rate": 0.00018806818181818182,
      "loss": 0.4142,
      "step": 110
    },
    {
      "epoch": 0.54421768707483,
      "grad_norm": 5.042272567749023,
      "learning_rate": 0.00018693181818181818,
      "loss": 0.4082,
      "step": 120
    },
    {
      "epoch": 0.5895691609977324,
      "grad_norm": 4.89505672454834,
      "learning_rate": 0.00018579545454545454,
      "loss": 0.3894,
      "step": 130
    },
    {
      "epoch": 0.6349206349206349,
      "grad_norm": 5.2901387214660645,
      "learning_rate": 0.00018465909090909093,
      "loss": 0.4038,
      "step": 140
    },
    {
      "epoch": 0.6802721088435374,
      "grad_norm": 5.189054489135742,
      "learning_rate": 0.00018352272727272727,
      "loss": 0.4061,
      "step": 150
    },
    {
      "epoch": 0.7256235827664399,
      "grad_norm": 5.467419147491455,
      "learning_rate": 0.00018238636363636365,
      "loss": 0.3885,
      "step": 160
    },
    {
      "epoch": 0.7709750566893424,
      "grad_norm": 4.965986728668213,
      "learning_rate": 0.00018125000000000001,
      "loss": 0.4165,
      "step": 170
    },
    {
      "epoch": 0.8163265306122449,
      "grad_norm": 5.5254950523376465,
      "learning_rate": 0.00018011363636363638,
      "loss": 0.411,
      "step": 180
    },
    {
      "epoch": 0.8616780045351474,
      "grad_norm": 4.835385799407959,
      "learning_rate": 0.00017897727272727274,
      "loss": 0.3843,
      "step": 190
    },
    {
      "epoch": 0.9070294784580499,
      "grad_norm": 5.012878894805908,
      "learning_rate": 0.0001778409090909091,
      "loss": 0.4007,
      "step": 200
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 5.49887752532959,
      "learning_rate": 0.00017670454545454546,
      "loss": 0.4083,
      "step": 210
    },
    {
      "epoch": 0.9977324263038548,
      "grad_norm": 4.801612854003906,
      "learning_rate": 0.00017556818181818182,
      "loss": 0.4134,
      "step": 220
    },
    {
      "epoch": 1.0430839002267573,
      "grad_norm": 5.339465141296387,
      "learning_rate": 0.0001744318181818182,
      "loss": 0.3654,
      "step": 230
    },
    {
      "epoch": 1.08843537414966,
      "grad_norm": 4.972084999084473,
      "learning_rate": 0.00017329545454545454,
      "loss": 0.3528,
      "step": 240
    },
    {
      "epoch": 1.1337868480725624,
      "grad_norm": 5.530465602874756,
      "learning_rate": 0.00017215909090909093,
      "loss": 0.3359,
      "step": 250
    },
    {
      "epoch": 1.179138321995465,
      "grad_norm": 5.665290355682373,
      "learning_rate": 0.0001710227272727273,
      "loss": 0.3586,
      "step": 260
    },
    {
      "epoch": 1.2244897959183674,
      "grad_norm": 4.2347002029418945,
      "learning_rate": 0.00016988636363636365,
      "loss": 0.3512,
      "step": 270
    },
    {
      "epoch": 1.2698412698412698,
      "grad_norm": 5.3450093269348145,
      "learning_rate": 0.00016875,
      "loss": 0.3668,
      "step": 280
    },
    {
      "epoch": 1.3151927437641724,
      "grad_norm": 5.532197952270508,
      "learning_rate": 0.00016761363636363637,
      "loss": 0.3598,
      "step": 290
    },
    {
      "epoch": 1.3605442176870748,
      "grad_norm": 4.827414035797119,
      "learning_rate": 0.00016647727272727273,
      "loss": 0.3455,
      "step": 300
    },
    {
      "epoch": 1.4058956916099774,
      "grad_norm": 4.811833381652832,
      "learning_rate": 0.0001653409090909091,
      "loss": 0.3406,
      "step": 310
    },
    {
      "epoch": 1.4512471655328798,
      "grad_norm": 5.623380661010742,
      "learning_rate": 0.00016420454545454548,
      "loss": 0.3454,
      "step": 320
    },
    {
      "epoch": 1.4965986394557822,
      "grad_norm": 4.980744361877441,
      "learning_rate": 0.0001630681818181818,
      "loss": 0.3562,
      "step": 330
    },
    {
      "epoch": 1.5419501133786848,
      "grad_norm": 5.1241984367370605,
      "learning_rate": 0.0001619318181818182,
      "loss": 0.3302,
      "step": 340
    },
    {
      "epoch": 1.5873015873015874,
      "grad_norm": 5.28137731552124,
      "learning_rate": 0.00016079545454545456,
      "loss": 0.3667,
      "step": 350
    },
    {
      "epoch": 1.6326530612244898,
      "grad_norm": 5.016243934631348,
      "learning_rate": 0.00015965909090909092,
      "loss": 0.3623,
      "step": 360
    },
    {
      "epoch": 1.6780045351473922,
      "grad_norm": 4.5827836990356445,
      "learning_rate": 0.00015852272727272728,
      "loss": 0.348,
      "step": 370
    },
    {
      "epoch": 1.7233560090702946,
      "grad_norm": 4.104240417480469,
      "learning_rate": 0.00015738636363636364,
      "loss": 0.3496,
      "step": 380
    },
    {
      "epoch": 1.7687074829931972,
      "grad_norm": 4.581586837768555,
      "learning_rate": 0.00015625,
      "loss": 0.3285,
      "step": 390
    },
    {
      "epoch": 1.8140589569160999,
      "grad_norm": 5.298330307006836,
      "learning_rate": 0.00015511363636363636,
      "loss": 0.3644,
      "step": 400
    },
    {
      "epoch": 1.8594104308390023,
      "grad_norm": 5.388826847076416,
      "learning_rate": 0.00015397727272727272,
      "loss": 0.3629,
      "step": 410
    },
    {
      "epoch": 1.9047619047619047,
      "grad_norm": 4.834232807159424,
      "learning_rate": 0.00015284090909090909,
      "loss": 0.3326,
      "step": 420
    },
    {
      "epoch": 1.9501133786848073,
      "grad_norm": 4.444230556488037,
      "learning_rate": 0.00015170454545454547,
      "loss": 0.3226,
      "step": 430
    },
    {
      "epoch": 1.99546485260771,
      "grad_norm": 4.901724815368652,
      "learning_rate": 0.0001505681818181818,
      "loss": 0.3408,
      "step": 440
    },
    {
      "epoch": 2.0408163265306123,
      "grad_norm": 4.2006916999816895,
      "learning_rate": 0.0001494318181818182,
      "loss": 0.3141,
      "step": 450
    },
    {
      "epoch": 2.0861678004535147,
      "grad_norm": 5.070992946624756,
      "learning_rate": 0.00014829545454545455,
      "loss": 0.2952,
      "step": 460
    },
    {
      "epoch": 2.131519274376417,
      "grad_norm": 5.087155818939209,
      "learning_rate": 0.00014715909090909092,
      "loss": 0.2754,
      "step": 470
    },
    {
      "epoch": 2.17687074829932,
      "grad_norm": 5.944204807281494,
      "learning_rate": 0.00014602272727272728,
      "loss": 0.3112,
      "step": 480
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 4.841128826141357,
      "learning_rate": 0.00014488636363636366,
      "loss": 0.3107,
      "step": 490
    },
    {
      "epoch": 2.2675736961451247,
      "grad_norm": 5.355216979980469,
      "learning_rate": 0.00014375,
      "loss": 0.3123,
      "step": 500
    },
    {
      "epoch": 2.312925170068027,
      "grad_norm": 4.929471969604492,
      "learning_rate": 0.00014261363636363636,
      "loss": 0.3017,
      "step": 510
    },
    {
      "epoch": 2.35827664399093,
      "grad_norm": 5.012671947479248,
      "learning_rate": 0.00014147727272727275,
      "loss": 0.2944,
      "step": 520
    },
    {
      "epoch": 2.4036281179138324,
      "grad_norm": 5.195387363433838,
      "learning_rate": 0.00014034090909090908,
      "loss": 0.305,
      "step": 530
    },
    {
      "epoch": 2.4489795918367347,
      "grad_norm": 5.807559490203857,
      "learning_rate": 0.00013920454545454547,
      "loss": 0.3073,
      "step": 540
    },
    {
      "epoch": 2.494331065759637,
      "grad_norm": 5.274169445037842,
      "learning_rate": 0.00013806818181818183,
      "loss": 0.2886,
      "step": 550
    },
    {
      "epoch": 2.5396825396825395,
      "grad_norm": 5.71915340423584,
      "learning_rate": 0.0001369318181818182,
      "loss": 0.2965,
      "step": 560
    },
    {
      "epoch": 2.5850340136054424,
      "grad_norm": 5.644324779510498,
      "learning_rate": 0.00013579545454545455,
      "loss": 0.2928,
      "step": 570
    },
    {
      "epoch": 2.630385487528345,
      "grad_norm": 5.12069034576416,
      "learning_rate": 0.00013465909090909094,
      "loss": 0.289,
      "step": 580
    },
    {
      "epoch": 2.675736961451247,
      "grad_norm": 5.772598743438721,
      "learning_rate": 0.00013352272727272727,
      "loss": 0.2986,
      "step": 590
    },
    {
      "epoch": 2.7210884353741496,
      "grad_norm": 5.189794063568115,
      "learning_rate": 0.00013238636363636366,
      "loss": 0.2931,
      "step": 600
    },
    {
      "epoch": 2.766439909297052,
      "grad_norm": 5.421967029571533,
      "learning_rate": 0.00013125000000000002,
      "loss": 0.2934,
      "step": 610
    },
    {
      "epoch": 2.811791383219955,
      "grad_norm": 5.222978591918945,
      "learning_rate": 0.00013011363636363635,
      "loss": 0.2893,
      "step": 620
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 5.100172519683838,
      "learning_rate": 0.00012897727272727274,
      "loss": 0.3087,
      "step": 630
    },
    {
      "epoch": 2.9024943310657596,
      "grad_norm": 4.798590660095215,
      "learning_rate": 0.0001278409090909091,
      "loss": 0.3012,
      "step": 640
    },
    {
      "epoch": 2.947845804988662,
      "grad_norm": 4.969069004058838,
      "learning_rate": 0.00012670454545454546,
      "loss": 0.3108,
      "step": 650
    },
    {
      "epoch": 2.9931972789115644,
      "grad_norm": 5.110987186431885,
      "learning_rate": 0.00012556818181818182,
      "loss": 0.2809,
      "step": 660
    },
    {
      "epoch": 3.0385487528344672,
      "grad_norm": 5.540356636047363,
      "learning_rate": 0.00012443181818181818,
      "loss": 0.2672,
      "step": 670
    },
    {
      "epoch": 3.0839002267573696,
      "grad_norm": 5.023423671722412,
      "learning_rate": 0.00012329545454545454,
      "loss": 0.2538,
      "step": 680
    },
    {
      "epoch": 3.129251700680272,
      "grad_norm": 5.534985065460205,
      "learning_rate": 0.00012215909090909093,
      "loss": 0.2375,
      "step": 690
    },
    {
      "epoch": 3.1746031746031744,
      "grad_norm": 6.3058648109436035,
      "learning_rate": 0.00012102272727272728,
      "loss": 0.2536,
      "step": 700
    },
    {
      "epoch": 3.2199546485260773,
      "grad_norm": 5.597710132598877,
      "learning_rate": 0.00011988636363636365,
      "loss": 0.2519,
      "step": 710
    },
    {
      "epoch": 3.2653061224489797,
      "grad_norm": 5.87182092666626,
      "learning_rate": 0.00011875,
      "loss": 0.2665,
      "step": 720
    },
    {
      "epoch": 3.310657596371882,
      "grad_norm": 5.376250267028809,
      "learning_rate": 0.00011761363636363636,
      "loss": 0.2574,
      "step": 730
    },
    {
      "epoch": 3.3560090702947845,
      "grad_norm": 4.910257816314697,
      "learning_rate": 0.00011647727272727273,
      "loss": 0.2481,
      "step": 740
    },
    {
      "epoch": 3.4013605442176873,
      "grad_norm": 5.891112327575684,
      "learning_rate": 0.00011534090909090908,
      "loss": 0.2452,
      "step": 750
    },
    {
      "epoch": 3.4467120181405897,
      "grad_norm": 5.624940872192383,
      "learning_rate": 0.00011420454545454547,
      "loss": 0.2548,
      "step": 760
    },
    {
      "epoch": 3.492063492063492,
      "grad_norm": 5.194700717926025,
      "learning_rate": 0.00011306818181818182,
      "loss": 0.2435,
      "step": 770
    },
    {
      "epoch": 3.5374149659863945,
      "grad_norm": 5.356083393096924,
      "learning_rate": 0.00011193181818181819,
      "loss": 0.2641,
      "step": 780
    },
    {
      "epoch": 3.582766439909297,
      "grad_norm": 5.2441511154174805,
      "learning_rate": 0.00011079545454545455,
      "loss": 0.2565,
      "step": 790
    },
    {
      "epoch": 3.6281179138321997,
      "grad_norm": 6.05649995803833,
      "learning_rate": 0.00010965909090909093,
      "loss": 0.2536,
      "step": 800
    },
    {
      "epoch": 3.673469387755102,
      "grad_norm": 6.725640773773193,
      "learning_rate": 0.00010852272727272727,
      "loss": 0.2647,
      "step": 810
    },
    {
      "epoch": 3.7188208616780045,
      "grad_norm": 5.4972405433654785,
      "learning_rate": 0.00010738636363636365,
      "loss": 0.2713,
      "step": 820
    },
    {
      "epoch": 3.764172335600907,
      "grad_norm": 5.450693607330322,
      "learning_rate": 0.00010625000000000001,
      "loss": 0.2603,
      "step": 830
    },
    {
      "epoch": 3.8095238095238093,
      "grad_norm": 5.98620080947876,
      "learning_rate": 0.00010511363636363635,
      "loss": 0.2479,
      "step": 840
    },
    {
      "epoch": 3.854875283446712,
      "grad_norm": 4.983320236206055,
      "learning_rate": 0.00010397727272727273,
      "loss": 0.2575,
      "step": 850
    },
    {
      "epoch": 3.9002267573696145,
      "grad_norm": 5.173401355743408,
      "learning_rate": 0.00010284090909090909,
      "loss": 0.2591,
      "step": 860
    },
    {
      "epoch": 3.945578231292517,
      "grad_norm": 5.509323596954346,
      "learning_rate": 0.00010170454545454546,
      "loss": 0.2502,
      "step": 870
    },
    {
      "epoch": 3.9909297052154193,
      "grad_norm": 6.471535682678223,
      "learning_rate": 0.00010056818181818181,
      "loss": 0.2423,
      "step": 880
    },
    {
      "epoch": 4.036281179138322,
      "grad_norm": 6.023395538330078,
      "learning_rate": 9.943181818181819e-05,
      "loss": 0.231,
      "step": 890
    },
    {
      "epoch": 4.081632653061225,
      "grad_norm": 5.371325492858887,
      "learning_rate": 9.829545454545455e-05,
      "loss": 0.208,
      "step": 900
    },
    {
      "epoch": 4.1269841269841265,
      "grad_norm": 6.353886127471924,
      "learning_rate": 9.71590909090909e-05,
      "loss": 0.2168,
      "step": 910
    },
    {
      "epoch": 4.172335600907029,
      "grad_norm": 5.545303821563721,
      "learning_rate": 9.602272727272728e-05,
      "loss": 0.22,
      "step": 920
    },
    {
      "epoch": 4.217687074829932,
      "grad_norm": 6.861306667327881,
      "learning_rate": 9.488636363636364e-05,
      "loss": 0.2155,
      "step": 930
    },
    {
      "epoch": 4.263038548752834,
      "grad_norm": 5.038594722747803,
      "learning_rate": 9.375e-05,
      "loss": 0.214,
      "step": 940
    },
    {
      "epoch": 4.308390022675737,
      "grad_norm": 6.817714691162109,
      "learning_rate": 9.261363636363636e-05,
      "loss": 0.2119,
      "step": 950
    },
    {
      "epoch": 4.35374149659864,
      "grad_norm": 7.247778415679932,
      "learning_rate": 9.147727272727274e-05,
      "loss": 0.2202,
      "step": 960
    },
    {
      "epoch": 4.399092970521542,
      "grad_norm": 5.585160732269287,
      "learning_rate": 9.03409090909091e-05,
      "loss": 0.2122,
      "step": 970
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 6.093752861022949,
      "learning_rate": 8.920454545454546e-05,
      "loss": 0.2018,
      "step": 980
    },
    {
      "epoch": 4.489795918367347,
      "grad_norm": 6.337399959564209,
      "learning_rate": 8.806818181818183e-05,
      "loss": 0.2164,
      "step": 990
    },
    {
      "epoch": 4.535147392290249,
      "grad_norm": 5.728371620178223,
      "learning_rate": 8.693181818181818e-05,
      "loss": 0.2158,
      "step": 1000
    },
    {
      "epoch": 4.580498866213152,
      "grad_norm": 6.635988235473633,
      "learning_rate": 8.579545454545454e-05,
      "loss": 0.2197,
      "step": 1010
    },
    {
      "epoch": 4.625850340136054,
      "grad_norm": 6.315345764160156,
      "learning_rate": 8.465909090909091e-05,
      "loss": 0.216,
      "step": 1020
    },
    {
      "epoch": 4.671201814058957,
      "grad_norm": 6.27715539932251,
      "learning_rate": 8.352272727272727e-05,
      "loss": 0.2259,
      "step": 1030
    },
    {
      "epoch": 4.71655328798186,
      "grad_norm": 6.021872520446777,
      "learning_rate": 8.238636363636364e-05,
      "loss": 0.221,
      "step": 1040
    },
    {
      "epoch": 4.761904761904762,
      "grad_norm": 5.791161060333252,
      "learning_rate": 8.125000000000001e-05,
      "loss": 0.2225,
      "step": 1050
    },
    {
      "epoch": 4.807256235827665,
      "grad_norm": 6.622839450836182,
      "learning_rate": 8.011363636363637e-05,
      "loss": 0.2083,
      "step": 1060
    },
    {
      "epoch": 4.852607709750567,
      "grad_norm": 5.641804218292236,
      "learning_rate": 7.897727272727273e-05,
      "loss": 0.2216,
      "step": 1070
    },
    {
      "epoch": 4.8979591836734695,
      "grad_norm": 7.165717601776123,
      "learning_rate": 7.784090909090909e-05,
      "loss": 0.2352,
      "step": 1080
    },
    {
      "epoch": 4.943310657596372,
      "grad_norm": 5.584473133087158,
      "learning_rate": 7.670454545454547e-05,
      "loss": 0.1992,
      "step": 1090
    },
    {
      "epoch": 4.988662131519274,
      "grad_norm": 5.584872722625732,
      "learning_rate": 7.556818181818183e-05,
      "loss": 0.2179,
      "step": 1100
    },
    {
      "epoch": 5.034013605442177,
      "grad_norm": 6.8569536209106445,
      "learning_rate": 7.443181818181817e-05,
      "loss": 0.1998,
      "step": 1110
    },
    {
      "epoch": 5.079365079365079,
      "grad_norm": 6.364053249359131,
      "learning_rate": 7.329545454545455e-05,
      "loss": 0.1782,
      "step": 1120
    },
    {
      "epoch": 5.124716553287982,
      "grad_norm": 5.938394069671631,
      "learning_rate": 7.215909090909091e-05,
      "loss": 0.179,
      "step": 1130
    },
    {
      "epoch": 5.170068027210885,
      "grad_norm": 6.345800876617432,
      "learning_rate": 7.102272727272727e-05,
      "loss": 0.1972,
      "step": 1140
    },
    {
      "epoch": 5.215419501133787,
      "grad_norm": 4.987888813018799,
      "learning_rate": 6.988636363636364e-05,
      "loss": 0.1921,
      "step": 1150
    },
    {
      "epoch": 5.26077097505669,
      "grad_norm": 7.882052898406982,
      "learning_rate": 6.875e-05,
      "loss": 0.1827,
      "step": 1160
    },
    {
      "epoch": 5.3061224489795915,
      "grad_norm": 4.2416558265686035,
      "learning_rate": 6.761363636363636e-05,
      "loss": 0.1787,
      "step": 1170
    },
    {
      "epoch": 5.351473922902494,
      "grad_norm": 5.635893821716309,
      "learning_rate": 6.647727272727274e-05,
      "loss": 0.1879,
      "step": 1180
    },
    {
      "epoch": 5.396825396825397,
      "grad_norm": 6.65172815322876,
      "learning_rate": 6.53409090909091e-05,
      "loss": 0.184,
      "step": 1190
    },
    {
      "epoch": 5.442176870748299,
      "grad_norm": 7.212499141693115,
      "learning_rate": 6.420454545454546e-05,
      "loss": 0.1994,
      "step": 1200
    },
    {
      "epoch": 5.487528344671202,
      "grad_norm": 7.3128557205200195,
      "learning_rate": 6.306818181818182e-05,
      "loss": 0.1873,
      "step": 1210
    },
    {
      "epoch": 5.532879818594104,
      "grad_norm": 6.275552749633789,
      "learning_rate": 6.193181818181818e-05,
      "loss": 0.1833,
      "step": 1220
    },
    {
      "epoch": 5.578231292517007,
      "grad_norm": 6.226970195770264,
      "learning_rate": 6.079545454545454e-05,
      "loss": 0.1896,
      "step": 1230
    },
    {
      "epoch": 5.62358276643991,
      "grad_norm": 7.064414978027344,
      "learning_rate": 5.965909090909091e-05,
      "loss": 0.1764,
      "step": 1240
    },
    {
      "epoch": 5.668934240362812,
      "grad_norm": 6.320387363433838,
      "learning_rate": 5.852272727272727e-05,
      "loss": 0.1947,
      "step": 1250
    },
    {
      "epoch": 5.714285714285714,
      "grad_norm": 5.543043613433838,
      "learning_rate": 5.738636363636364e-05,
      "loss": 0.1937,
      "step": 1260
    },
    {
      "epoch": 5.759637188208616,
      "grad_norm": 5.853797435760498,
      "learning_rate": 5.6250000000000005e-05,
      "loss": 0.1874,
      "step": 1270
    },
    {
      "epoch": 5.804988662131519,
      "grad_norm": 5.653610706329346,
      "learning_rate": 5.5113636363636366e-05,
      "loss": 0.1833,
      "step": 1280
    },
    {
      "epoch": 5.850340136054422,
      "grad_norm": 7.066866874694824,
      "learning_rate": 5.397727272727273e-05,
      "loss": 0.1921,
      "step": 1290
    },
    {
      "epoch": 5.895691609977324,
      "grad_norm": 6.1764631271362305,
      "learning_rate": 5.2840909090909094e-05,
      "loss": 0.1887,
      "step": 1300
    },
    {
      "epoch": 5.941043083900227,
      "grad_norm": 6.5792341232299805,
      "learning_rate": 5.170454545454546e-05,
      "loss": 0.1822,
      "step": 1310
    },
    {
      "epoch": 5.986394557823129,
      "grad_norm": 6.4741902351379395,
      "learning_rate": 5.056818181818183e-05,
      "loss": 0.1837,
      "step": 1320
    },
    {
      "epoch": 6.031746031746032,
      "grad_norm": 5.893185615539551,
      "learning_rate": 4.943181818181818e-05,
      "loss": 0.1733,
      "step": 1330
    },
    {
      "epoch": 6.0770975056689345,
      "grad_norm": 5.218473434448242,
      "learning_rate": 4.829545454545455e-05,
      "loss": 0.1614,
      "step": 1340
    },
    {
      "epoch": 6.122448979591836,
      "grad_norm": 5.451981067657471,
      "learning_rate": 4.715909090909091e-05,
      "loss": 0.1519,
      "step": 1350
    },
    {
      "epoch": 6.167800453514739,
      "grad_norm": 6.258353233337402,
      "learning_rate": 4.602272727272727e-05,
      "loss": 0.169,
      "step": 1360
    },
    {
      "epoch": 6.213151927437642,
      "grad_norm": 5.810448169708252,
      "learning_rate": 4.488636363636364e-05,
      "loss": 0.1614,
      "step": 1370
    },
    {
      "epoch": 6.258503401360544,
      "grad_norm": 7.118003845214844,
      "learning_rate": 4.375e-05,
      "loss": 0.1679,
      "step": 1380
    },
    {
      "epoch": 6.303854875283447,
      "grad_norm": 6.173035144805908,
      "learning_rate": 4.261363636363637e-05,
      "loss": 0.1628,
      "step": 1390
    },
    {
      "epoch": 6.349206349206349,
      "grad_norm": 6.878085613250732,
      "learning_rate": 4.1477272727272734e-05,
      "loss": 0.1622,
      "step": 1400
    },
    {
      "epoch": 6.394557823129252,
      "grad_norm": 5.571707725524902,
      "learning_rate": 4.034090909090909e-05,
      "loss": 0.154,
      "step": 1410
    },
    {
      "epoch": 6.4399092970521545,
      "grad_norm": 7.022226333618164,
      "learning_rate": 3.9204545454545456e-05,
      "loss": 0.1636,
      "step": 1420
    },
    {
      "epoch": 6.4852607709750565,
      "grad_norm": 6.252140045166016,
      "learning_rate": 3.8068181818181816e-05,
      "loss": 0.1671,
      "step": 1430
    },
    {
      "epoch": 6.530612244897959,
      "grad_norm": 5.953193187713623,
      "learning_rate": 3.6931818181818184e-05,
      "loss": 0.1752,
      "step": 1440
    },
    {
      "epoch": 6.575963718820862,
      "grad_norm": 7.010894298553467,
      "learning_rate": 3.579545454545455e-05,
      "loss": 0.1698,
      "step": 1450
    },
    {
      "epoch": 6.621315192743764,
      "grad_norm": 6.738468170166016,
      "learning_rate": 3.465909090909091e-05,
      "loss": 0.1688,
      "step": 1460
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 6.308309078216553,
      "learning_rate": 3.352272727272727e-05,
      "loss": 0.1672,
      "step": 1470
    },
    {
      "epoch": 6.712018140589569,
      "grad_norm": 6.522159099578857,
      "learning_rate": 3.238636363636364e-05,
      "loss": 0.1623,
      "step": 1480
    },
    {
      "epoch": 6.757369614512472,
      "grad_norm": 5.795232772827148,
      "learning_rate": 3.125e-05,
      "loss": 0.172,
      "step": 1490
    },
    {
      "epoch": 6.802721088435375,
      "grad_norm": 6.887721061706543,
      "learning_rate": 3.0113636363636365e-05,
      "loss": 0.1577,
      "step": 1500
    },
    {
      "epoch": 6.8480725623582765,
      "grad_norm": 5.339073657989502,
      "learning_rate": 2.8977272727272732e-05,
      "loss": 0.1634,
      "step": 1510
    },
    {
      "epoch": 6.893424036281179,
      "grad_norm": 6.91414213180542,
      "learning_rate": 2.784090909090909e-05,
      "loss": 0.162,
      "step": 1520
    },
    {
      "epoch": 6.938775510204081,
      "grad_norm": 5.772916316986084,
      "learning_rate": 2.6704545454545453e-05,
      "loss": 0.1553,
      "step": 1530
    },
    {
      "epoch": 6.984126984126984,
      "grad_norm": 5.58880615234375,
      "learning_rate": 2.5568181818181817e-05,
      "loss": 0.158,
      "step": 1540
    },
    {
      "epoch": 7.029478458049887,
      "grad_norm": 4.941119194030762,
      "learning_rate": 2.4431818181818185e-05,
      "loss": 0.1438,
      "step": 1550
    },
    {
      "epoch": 7.074829931972789,
      "grad_norm": 7.234124183654785,
      "learning_rate": 2.3295454545454546e-05,
      "loss": 0.1479,
      "step": 1560
    },
    {
      "epoch": 7.120181405895692,
      "grad_norm": 5.339208126068115,
      "learning_rate": 2.215909090909091e-05,
      "loss": 0.1538,
      "step": 1570
    },
    {
      "epoch": 7.165532879818594,
      "grad_norm": 5.374173641204834,
      "learning_rate": 2.1022727272727274e-05,
      "loss": 0.1411,
      "step": 1580
    },
    {
      "epoch": 7.210884353741497,
      "grad_norm": 5.705055236816406,
      "learning_rate": 1.9886363636363638e-05,
      "loss": 0.1441,
      "step": 1590
    },
    {
      "epoch": 7.2562358276643995,
      "grad_norm": 5.702969551086426,
      "learning_rate": 1.8750000000000002e-05,
      "loss": 0.1392,
      "step": 1600
    },
    {
      "epoch": 7.301587301587301,
      "grad_norm": 6.388431072235107,
      "learning_rate": 1.7613636363636366e-05,
      "loss": 0.1484,
      "step": 1610
    },
    {
      "epoch": 7.346938775510204,
      "grad_norm": 5.126889228820801,
      "learning_rate": 1.6477272727272726e-05,
      "loss": 0.153,
      "step": 1620
    },
    {
      "epoch": 7.392290249433106,
      "grad_norm": 6.5535101890563965,
      "learning_rate": 1.534090909090909e-05,
      "loss": 0.1427,
      "step": 1630
    },
    {
      "epoch": 7.437641723356009,
      "grad_norm": 5.950042724609375,
      "learning_rate": 1.4204545454545456e-05,
      "loss": 0.1524,
      "step": 1640
    },
    {
      "epoch": 7.482993197278912,
      "grad_norm": 5.374727249145508,
      "learning_rate": 1.3068181818181819e-05,
      "loss": 0.1446,
      "step": 1650
    },
    {
      "epoch": 7.528344671201814,
      "grad_norm": 6.6217122077941895,
      "learning_rate": 1.1931818181818183e-05,
      "loss": 0.1511,
      "step": 1660
    },
    {
      "epoch": 7.573696145124717,
      "grad_norm": 5.525487899780273,
      "learning_rate": 1.0795454545454547e-05,
      "loss": 0.1486,
      "step": 1670
    },
    {
      "epoch": 7.619047619047619,
      "grad_norm": 6.570509433746338,
      "learning_rate": 9.659090909090909e-06,
      "loss": 0.1497,
      "step": 1680
    },
    {
      "epoch": 7.6643990929705215,
      "grad_norm": 6.043491840362549,
      "learning_rate": 8.522727272727273e-06,
      "loss": 0.1482,
      "step": 1690
    },
    {
      "epoch": 7.709750566893424,
      "grad_norm": 6.367557048797607,
      "learning_rate": 7.386363636363637e-06,
      "loss": 0.1534,
      "step": 1700
    },
    {
      "epoch": 7.755102040816326,
      "grad_norm": 5.551109790802002,
      "learning_rate": 6.25e-06,
      "loss": 0.1429,
      "step": 1710
    },
    {
      "epoch": 7.800453514739229,
      "grad_norm": 6.210136413574219,
      "learning_rate": 5.113636363636364e-06,
      "loss": 0.1484,
      "step": 1720
    },
    {
      "epoch": 7.845804988662131,
      "grad_norm": 5.1595540046691895,
      "learning_rate": 3.9772727272727275e-06,
      "loss": 0.1416,
      "step": 1730
    },
    {
      "epoch": 7.891156462585034,
      "grad_norm": 5.694505214691162,
      "learning_rate": 2.840909090909091e-06,
      "loss": 0.1444,
      "step": 1740
    },
    {
      "epoch": 7.936507936507937,
      "grad_norm": 5.39713191986084,
      "learning_rate": 1.7045454545454546e-06,
      "loss": 0.1421,
      "step": 1750
    },
    {
      "epoch": 7.981859410430839,
      "grad_norm": 5.698692798614502,
      "learning_rate": 5.681818181818182e-07,
      "loss": 0.1424,
      "step": 1760
    }
  ],
  "logging_steps": 10,
  "max_steps": 1760,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 8,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.41643973046272e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
